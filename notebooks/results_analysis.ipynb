{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm # Используем версию для ноутбуков\n",
    "from sklearn.metrics import confusion_matrix\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from config import config as main_config \n",
    "from utils.metrics import calculate_levenshtein_mean, calculate_cer\n",
    "from utils.decoding import decode_predictions, decode_targets\n",
    "from data.dataset import MorseDataset \n",
    "from data.collate import collate_fn \n",
    "from models.crnn import CRNNModel_4Layer\n",
    "\n",
    "log_file_path = main_config.LOG_FILE\n",
    "history_df = None\n",
    "\n",
    "try:\n",
    "    history_df = pd.read_csv(log_file_path)\n",
    "    print(f\"Лог файл загружен: {log_file_path}\")\n",
    "    display(history_df.head())\n",
    "\n",
    "    # Строим графики\n",
    "    epochs_range = history_df['Epoch']\n",
    "\n",
    "    plt.figure(figsize=(18, 5))\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.plot(epochs_range, history_df['Train Loss'], label='Train Loss')\n",
    "    plt.plot(epochs_range, history_df['Val Loss'], label='Validation Loss')\n",
    "    plt.legend(loc='best'); plt.title('Loss'); plt.xlabel('Epoch'); plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.plot(epochs_range, history_df['Val Levenshtein'], label='Validation Levenshtein')\n",
    "    plt.legend(loc='best'); plt.title('Levenshtein Distance'); plt.xlabel('Epoch'); plt.grid(True)\n",
    "    best_lev_epoch = history_df.loc[history_df['Val Levenshtein'].idxmin()]\n",
    "    plt.scatter(best_lev_epoch['Epoch'], best_lev_epoch['Val Levenshtein'], color='red', s=50, label=f'Best: {best_lev_epoch[\"Val Levenshtein\"]:.4f} (Ep {int(best_lev_epoch[\"Epoch\"])})')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    if 'Val CER' in history_df.columns and not history_df['Val CER'].isnull().all():\n",
    "         plt.plot(epochs_range, history_df['Val CER'], label='Validation CER')\n",
    "         plt.legend(loc='best'); plt.title('Character Error Rate (CER)'); plt.xlabel('Epoch'); plt.grid(True)\n",
    "    else:\n",
    "         plt.text(0.5, 0.5, 'CER data not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "         plt.title('Character Error Rate (CER)')\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    if 'LR' in history_df.columns:\n",
    "        plt.plot(epochs_range, history_df['LR'], label='Learning Rate')\n",
    "        plt.legend(loc='best'); plt.title('Learning Rate'); plt.xlabel('Epoch'); plt.grid(True); plt.yscale('log') \n",
    "    else:\n",
    "         plt.text(0.5, 0.5, 'LR data not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "         plt.title('Learning Rate')\n",
    "\n",
    "\n",
    "    plt.suptitle(f'Результаты обучения модели: {main_config.MODEL_NAME}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ошибка: Лог файл не найден по пути {log_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке или отрисовке логов: {e}\")\n",
    "\n",
    "DEVICE = main_config.DEVICE\n",
    "best_model = None\n",
    "best_checkpoint_path = None\n",
    "loaded_config = None\n",
    "char_to_int = None\n",
    "int_to_char = None\n",
    "blank_index = -1\n",
    "\n",
    "try:\n",
    "    best_checkpoint_path = os.path.join(main_config.CHECKPOINT_DIR, f\"{main_config.MODEL_NAME}_best.pth\")\n",
    "    if not os.path.exists(best_checkpoint_path):\n",
    "         available_checkpoints = [f for f in os.listdir(main_config.CHECKPOINT_DIR) if f.endswith(\".pth\") and main_config.MODEL_NAME in f]\n",
    "         if not available_checkpoints:\n",
    "             raise FileNotFoundError(f\"Чекпоинты не найдены в {main_config.CHECKPOINT_DIR}\")\n",
    "         best_checkpoint_path = os.path.join(main_config.CHECKPOINT_DIR, available_checkpoints[0])\n",
    "         print(f\"Файл best.pth не найден, используется: {os.path.basename(best_checkpoint_path)}\")\n",
    "\n",
    "    print(f\"\\nЗагрузка лучшей модели для анализа: {os.path.basename(best_checkpoint_path)}\")\n",
    "    checkpoint = torch.load(best_checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "    if 'config' not in checkpoint or 'char_to_int' not in checkpoint or 'int_to_char' not in checkpoint or 'blank_index' not in checkpoint:\n",
    "        raise ValueError(\"Необходимые данные (config, char_map, blank_index) не найдены в чекпоинте!\")\n",
    "\n",
    "    loaded_config = checkpoint['config']\n",
    "    char_to_int = checkpoint['char_to_int']\n",
    "    int_to_char = checkpoint['int_to_char']\n",
    "    blank_index = checkpoint['blank_index']\n",
    "    NUM_CLASSES = len(char_to_int)\n",
    "\n",
    "    print(\"Конфигурация модели из чекпоинта:\")\n",
    "    print(f\"  Тип: {loaded_config.get('MODEL', {}).get('type', 'N/A')}\")\n",
    "    print(f\"  Размер RNN: {loaded_config.get('MODEL', {}).get('rnn_hidden_size', 'N/A')}\")\n",
    "    print(f\"  Слои RNN: {loaded_config.get('MODEL', {}).get('rnn_num_layers', 'N/A')}\")\n",
    "    print(f\"  n_mels: {loaded_config.get('AUDIO', {}).get('n_mels', 'N/A')}\")\n",
    "\n",
    "    model_cfg = loaded_config['MODEL']\n",
    "    audio_cfg = loaded_config['AUDIO']\n",
    "\n",
    "    if model_cfg['type'] == 'CRNNModel_4Layer':\n",
    "        best_model = CRNNModel_4Layer(\n",
    "            n_features=audio_cfg['n_mels'], num_classes=NUM_CLASSES,\n",
    "            rnn_hidden_size=model_cfg['rnn_hidden_size'], num_rnn_layers=model_cfg['rnn_num_layers'],\n",
    "            cnn_dropout=0.0, rnn_dropout=0.0 \n",
    "        ).to(DEVICE)\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный тип модели '{model_cfg['type']}' в чекпоинте\")\n",
    "\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    best_model.eval() \n",
    "    print(f\"Лучшая модель (Эпоха {checkpoint.get('epoch', 'N/A')}, Metric: {checkpoint.get('val_metric', 'N/A'):.4f}) загружена.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ошибка: Чекпоинт не найден в '{main_config.CHECKPOINT_DIR}'\")\n",
    "    best_model = None \n",
    "except Exception as e:\n",
    "    print(f\"Ошибка загрузки лучшей модели: {e}\")\n",
    "    traceback.print_exc()\n",
    "    best_model = None\n",
    "\n",
    "val_loader_final = None\n",
    "if best_model and loaded_config and char_to_int:\n",
    "    print(\"\\nПодготовка валидационного лоадера для финальной оценки...\")\n",
    "    try:\n",
    "        train_df_full = pd.read_csv(main_config.TRAIN_CSV_PATH)\n",
    "        train_df_full[main_config.TARGET_COLUMN] = train_df_full[main_config.TARGET_COLUMN].fillna('').astype(str)\n",
    "        train_df_filtered = train_df_full[train_df_full[main_config.TARGET_COLUMN].str.len() > 0].copy()\n",
    "\n",
    "        # Используем тот же random_state для split, чтобы получить ту же val выборку\n",
    "        _, val_df_final = train_test_split(\n",
    "            train_df_filtered,\n",
    "            test_size=main_config.TRAINING['validation_split_size'], # Используем размер сплита из основного конфига\n",
    "            random_state=main_config.SEED\n",
    "        )\n",
    "        val_df_final = val_df_final.reset_index(drop=True)\n",
    "\n",
    "        # Создаем датасет с параметрами из ЗАГРУЖЕННОГО конфига\n",
    "        val_dataset_final = MorseDataset(\n",
    "            data_frame=val_df_final, audio_base_path=main_config.AUDIO_BASE_PATH,\n",
    "            char_map=char_to_int, # Используем словарь из чекпоинта\n",
    "            file_path_column=loaded_config['FILE_PATH_COLUMN'],\n",
    "            target_column=loaded_config['TARGET_COLUMN'],\n",
    "            test_id_column=loaded_config['TEST_ID_COLUMN'],\n",
    "            audio_cfg=loaded_config[\"AUDIO\"],\n",
    "            preproc_cfg=loaded_config[\"PREPROCESSING\"],\n",
    "            aug_cfg=None,\n",
    "            is_train=False\n",
    "        )\n",
    "\n",
    "        val_loader_final = DataLoader(\n",
    "            val_dataset_final, batch_size=main_config.INFERENCE['batch_size'], \n",
    "            shuffle=False, collate_fn=collate_fn, num_workers=0\n",
    "        )\n",
    "        print(f\"Валидационный лоадер создан ({len(val_dataset_final)} сэмплов).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка создания валидационного лоадера: {e}\")\n",
    "        val_loader_final = None\n",
    "\n",
    "\n",
    "if best_model and val_loader_final and int_to_char and blank_index != -1:\n",
    "    print(\"\\nРасчет финальных метрик и Confusion Matrix на валидационной выборке (Greedy Decode)...\")\n",
    "    all_val_preds_final, all_val_targets_final = [], []\n",
    "    val_iter_final = tqdm(val_loader_final, desc=\"Final Validation\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter_final:\n",
    "            try:\n",
    "                if not isinstance(batch, (tuple, list)) or len(batch) != 4: continue\n",
    "                spec_batch, spec_len_batch, target_batch, target_len_batch = batch\n",
    "                if spec_batch.numel() == 0: continue\n",
    "\n",
    "                spec_batch = spec_batch.to(DEVICE)\n",
    "                spec_len_batch = spec_len_batch.to(DEVICE)\n",
    "                target_batch = target_batch.to(DEVICE)\n",
    "                target_len_batch = target_len_batch.to(DEVICE)\n",
    "\n",
    "\n",
    "                log_probs, output_lengths = best_model(spec_batch, spec_len_batch)\n",
    "                output_lengths = torch.clamp(output_lengths, max=log_probs.shape[0])\n",
    "\n",
    "                batch_preds = decode_predictions(log_probs, int_to_char, blank_index)\n",
    "                batch_targets = decode_targets(target_batch.cpu(), target_len_batch.cpu(), int_to_char) \n",
    "                all_val_preds_final.extend(batch_preds)\n",
    "                all_val_targets_final.extend(batch_targets)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nОшибка на шаге финальной валидации: {e}\")\n",
    "                continue\n",
    "\n",
    "    if all_val_targets_final:\n",
    "        final_lev = calculate_levenshtein_mean(all_val_preds_final, all_val_targets_final)\n",
    "        final_cer = calculate_cer(all_val_preds_final, all_val_targets_final)\n",
    "        print(f\"\\nФинальные метрики на Validation Set (Best Model, Greedy Decode):\")\n",
    "        print(f\"  - Levenshtein Distance: {final_lev:.4f}\")\n",
    "        print(f\"  - Character Error Rate (CER): {final_cer:.4f}\")\n",
    "\n",
    "        try:\n",
    "            print(\"\\nПостроение Confusion Matrix...\")\n",
    "            pred_chars = \"\".join(all_val_preds_final)\n",
    "            target_chars = \"\".join(all_val_targets_final)\n",
    "\n",
    "            max_len_cm = 100000\n",
    "            if len(target_chars) > max_len_cm:\n",
    "                 print(f\"(Слишком много символов ({len(target_chars)}), матрица строится на {max_len_cm} случайных)\")\n",
    "                 indices = np.random.choice(len(target_chars), max_len_cm, replace=False)\n",
    "                 pred_chars_list = list(pred_chars)\n",
    "                 target_chars_list = list(target_chars)\n",
    "                 pred_chars = \"\".join([pred_chars_list[i] for i in indices])\n",
    "                 target_chars = \"\".join([target_chars_list[i] for i in indices])\n",
    "\n",
    "\n",
    "            labels = sorted(list(set(target_chars) | set(pred_chars)))\n",
    "            if not labels:\n",
    "                 print(\"Нет символов для построения матрицы ошибок.\")\n",
    "            else:\n",
    "                print(f\"Уникальные символы для матрицы ({len(labels)}): {''.join(labels)}\")\n",
    "                cm = confusion_matrix(list(target_chars), list(pred_chars), labels=labels)\n",
    "                cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "                plt.figure(figsize=(max(10, len(labels)*0.4), max(8, len(labels)*0.35))) \n",
    "                sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "                plt.title(f'Confusion Matrix - Validation Set ({main_config.MODEL_NAME})')\n",
    "                plt.ylabel('Actual Characters')\n",
    "                plt.xlabel('Predicted Characters')\n",
    "                plt.show()\n",
    "\n",
    "                errors = []\n",
    "                total_errors = 0\n",
    "                for i, label_true in enumerate(labels):\n",
    "                    for j, label_pred in enumerate(labels):\n",
    "                        if i != j and cm[i, j] > 0:\n",
    "                            errors.append(((label_true, label_pred), cm[i, j]))\n",
    "                            total_errors += cm[i,j]\n",
    "\n",
    "                if errors:\n",
    "                    errors.sort(key=lambda item: item[1], reverse=True)\n",
    "                    print(f\"\\nНаиболее частые ошибки ({total_errors} всего ошибок):\")\n",
    "                    for pair, count in errors[:20]: # Показать топ N\n",
    "                        true_char = pair[0] if pair[0] != ' ' else \"' '\"\n",
    "                        pred_char = pair[1] if pair[1] != ' ' else \"' '\"\n",
    "                        print(f\"  {true_char} -> {pred_char}: {count}\")\n",
    "                else:\n",
    "                     print(\"\\nОшибок не найдено!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка построения Confusion Matrix: {e}\")\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "         print(\"Не удалось получить предсказания/таргеты для финального анализа.\")\n",
    "elif not best_model:\n",
    "    print(\"Анализ невозможен, так как лучшая модель не была загружена.\")\n",
    "else:\n",
    "    print(\"Анализ невозможен, так как не удалось создать валидационный лоадер или отсутствуют словари.\")\n",
    "\n",
    "# +\n",
    "# Опционально: Показать примеры предсказаний\n",
    "if 'all_val_preds_final' in locals() and 'all_val_targets_final' in locals() and all_val_targets_final:\n",
    "    print(\"\\nПримеры предсказаний на валидационной выборке:\")\n",
    "    num_samples_to_show = 15\n",
    "    indices_to_show = np.random.choice(len(all_val_targets_final), min(num_samples_to_show, len(all_val_targets_final)), replace=False)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    for i in indices_to_show:\n",
    "        target = all_val_targets_final[i]\n",
    "        pred = all_val_preds_final[i]\n",
    "        is_correct = \"(Correct)\" if target == pred else \"(Incorrect)\"\n",
    "        print(f\"Target:    {target}\")\n",
    "        print(f\"Predicted: {pred} {is_correct}\")\n",
    "        print(\"-\" * 50)\n",
    "# -"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
